import pandas as pd
import numpy as np
import json
import pickle
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, classification_report
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier

# Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªÙŠØ±Ø§Ø¯ LightGBM Ùˆ XGBoost
try:
    from xgboost import XGBClassifier
    xgboost_available = True
except:
    xgboost_available = False

try:
    import lightgbm as lgb
    lightgbm_available = True
except:
    lightgbm_available = False


# =====================================================
# 1) ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
# =====================================================

df = pd.read_csv("external_traffic.csv")

print("\nğŸ“¥ ØªÙ… ØªØ­Ù…ÙŠÙ„ external_traffic.csv")
print("Ø¹Ø¯Ø¯ Ø§Ù„Ø³Ø¬Ù„Ø§Øª:", len(df))


# =====================================================
# 2) ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
# =====================================================

features = ["lat", "lng", "day", "hour", "traffic_num"]
target = "risk_label"

X = df[features]
y = df[target]

# ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª 80/20
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print("\nğŸ“Š ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:")
print("Training:", len(X_train), " | Test:", len(X_test))


# =====================================================
# 3) Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø±Ø´Ø­Ø©
# =====================================================

models = {
    "RandomForest": RandomForestClassifier(
        n_estimators=300, max_depth=12, random_state=42
    ),

    "GradientBoosting": GradientBoostingClassifier(),

    "LogisticRegression": LogisticRegression(max_iter=200),

    "MLP_NeuralNet": MLPClassifier(hidden_layer_sizes=(64, 32),
                                   max_iter=400, random_state=42)
}

# Ø¥Ø¶Ø§ÙØ© XGBoost Ø¥Ø°Ø§ Ù…ØªÙˆÙØ±
if xgboost_available:
    models["XGBoost"] = XGBClassifier(
        n_estimators=400,
        learning_rate=0.05,
        max_depth=6,
        subsample=0.9,
        colsample_bytree=0.9,
        eval_metric="mlogloss"
    )

# Ø¥Ø¶Ø§ÙØ© LightGBM Ø¥Ø°Ø§ Ù…ØªÙˆÙØ±
if lightgbm_available:
    models["LightGBM"] = lgb.LGBMClassifier(
        n_estimators=300,
        learning_rate=0.05,
        max_depth=-1
    )


# =====================================================
# 4) ØªØ¯Ø±ÙŠØ¨ ÙƒÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø£ÙØ¶Ù„
# =====================================================

best_model = None
best_f1 = -1
metrics_dict = {}

print("\nğŸ” Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬...\n")

for name, model in models.items():
    print(f"ğŸš€ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {name}")

    try:
        model.fit(X_train, y_train)
        preds = model.predict(X_test)

        acc = accuracy_score(y_test, preds)
        f1 = f1_score(y_test, preds, average="macro")

        print(f"  â†’ Accuracy = {acc:.4f}")
        print(f"  â†’ Macro F1 = {f1:.4f}\n")

        metrics_dict[name] = {
            "accuracy": acc,
            "f1_macro": f1,
            "classification_report": classification_report(
                y_test, preds, output_dict=False
            )
        }

        if f1 > best_f1:
            best_f1 = f1
            best_model = model
            best_model_name = name

    except Exception as e:
        print(f"âŒ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ {name} ÙØ´Ù„: {e}\n")


print("\n========================================")
print(f"ğŸ† Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬: {best_model_name}")
print(f"ğŸ”¢ Macro F1: {best_f1:.4f}")
print("========================================\n")


# =====================================================
# 5) Ø­ÙØ¸ Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬
# =====================================================

with open("model.pkl", "wb") as f:
    pickle.dump(best_model, f)

print("ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ model.pkl")


# =====================================================
# 6) Ø­ÙØ¸ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ ÙÙŠ Ù…Ù„Ù JSON
# =====================================================

with open("metrics.json", "w", encoding="utf-8") as f:
    json.dump(metrics_dict, f, indent=4, ensure_ascii=False)

print("ğŸ“Š ØªÙ… Ø­ÙØ¸ Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡ ÙÙŠ metrics.json\n")


print("ğŸ‰ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§ÙƒØªÙ…Ù„ Ø¨Ù†Ø¬Ø§Ø­! Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¬Ø§Ù‡Ø² Ù„Ù„Ø¹Ù…Ù„ ÙÙŠ Ù…Ø´Ø±ÙˆØ¹ AmanAI.")
